{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6\n",
    "Mikhail Belyaev and Maxim Panov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bongard problem\n",
    "* Present two sets of relatively simple diagrams, say A and B.\n",
    "* All the diagrams from set A have a common factor or attribute, which is lacking in all the diagrams of set B.\n",
    "* The problem is to find, or to formulate, convincingly, the common factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](./bongard/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./bongard/6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./bongard/12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./bongard/16.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./bongard/27.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](./bongard/44.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](./bongard/50.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](./bongard/69.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Toy Classification Problem\n",
    "\n",
    "* Data $X$ and $Y$ , with $Y$ taking values $+1$ or $−1$.\n",
    "* Here $X = (X_1, X_2)$.\n",
    "* The black boundary is the **Bayes Decision Boundary** - the best one can do.\n",
    "* **Goal:** Given $N$ training pairs $(X_i, Y_i)$ produce a classifier $\\hat{C}(X) \\in \\{-1, 1\\}$.\n",
    "\n",
    "![image](./figures/toy_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Deterministic problem noise comes from sampling distribution of $X$.\n",
    "* Use a training sample of size 200.\n",
    "* Here **Bayes Error** is $0 \\%$.\n",
    "\n",
    "![image](./figures/toy_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Classification tree provides a specific type of decisoin boundary.\n",
    "* When the sphere is in 10-dimensions, Classification Trees produces a rather noisy and inaccurate rule $\\hat{C}(X)$ with error rates around $30\\%$.\n",
    "\n",
    "![image](./figures/toy_3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Classification trees can be simple, but often produce noisy or weak classifiers.\n",
    "\n",
    "* **Bagging** (Breiman, 1996): Fit many large trees to bootstrap-resampled versions of the training data, and classify by majority vote.\n",
    "* **Boosting** (Freund & Shapire, 1996): Fit many large or small trees to reweighted versions of the training data. Classify by weighted majority vote.\n",
    "* **Random Forests** (Breiman 1999): Fancier version of bagging.\n",
    "\n",
    "In general Boosting > Random Forests > Bagging > Single Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "* Suppose $C(S, x)$ is a classifier, such as a tree, based on our training data $S$, producing a predicted class label at input point $x$.\n",
    "* To bag $C$, we draw bootstrap samples $S_1, \\dots, S_B$ each of size $N$ with replacement from the training data. \n",
    "* Then $\\hat{C}_{bag}(x) = Majority Vote \\{C(S_b, x)\\}_{b = 1}^B$.\n",
    "\n",
    "* Bagging can dramatically reduce the variance of unstable procedures (like trees), leading to improved prediction. \n",
    "* However any simple structure in $C$ (e.g a tree) is lost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bagging averages many trees, and produces smoother decision boundaries.\n",
    "\n",
    "![image](./figures/toy_bagging.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Random Forests\n",
    "* Refinement of bagged trees; quite popular.\n",
    "* At each tree split, a random sample of $m$ features is drawn, and only those $m$ features are considered for splitting. \n",
    "* For each tree grown on a bootstrap sample, the error rate for observations left out of the bootstrap sample is monitored. This is called the “out-of-bag” error rate.\n",
    "* Random forests tries to improve on bagging by “de-correlating” the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parameters of Random Forests\n",
    "\n",
    "* *n_estimators* - the number of trees in the forest.\n",
    "* *max_features* - the number of features to consider when looking for the best split.\n",
    "* *bootstrap* - whether bootstrap samples are used when building trees.\n",
    "* All the parameters of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Boosting\n",
    "\n",
    "* Average many trees, each grown to re-weighted versions of the training data.\n",
    "* Final Classifier is weighted average of classifiers: $C(x) = sign \\Bigl[\\sum_{m=1}^M \\alpha_m C_m(x)\\Bigr]$.\n",
    "* One of the most popular boosting schemes: *Gradient Boosting*.\n",
    "\n",
    "![image](./figures/boosting.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands On: Kaggle's 'Forest Cover Type Prediction' competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read in the data as pandas dataframes. Data was downloaded as csv files from the [Kaggle competition Data page](http://www.kaggle.com/c/forest-cover-type-prediction/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('forest_train.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The 'Ids' column will not be required as a feature, so let's drop it from the train dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train = train.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Seperate training data into features (xtrain) and targets (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = train['Cover_Type']\n",
    "X = train.drop('Cover_Type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(cross_validation.cross_val_score(clf_dt, X, y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TODO\n",
    " - Use RandomForest \n",
    " - Find optimal number of trees\n",
    " - Try Gradient boosting\n",
    " - Do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
